============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-7.4.2, pluggy-1.3.0
rootdir: /mnt/sda/vllm
plugins: anyio-4.2.0, flakefinder-1.1.0, hypothesis-5.35.1, xdist-3.3.1, rerunfailures-12.0, shard-0.1.2, xdoctest-1.0.2
collected 1 item
Running 1 items in this shard

tests/spec_decode/e2e/test_integration.py F                              [100%]

=================================== FAILURES ===================================
_ test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0] _

baseline_llm_generator = <function create_llm_generator.<locals>.generator_outer at 0x7f7ebe470700>
test_llm_generator = <function create_llm_generator.<locals>.generator_outer at 0x7f7ebe4708b0>
batch_size = 8, output_len = 32

    @pytest.mark.parametrize(
        "common_llm_kwargs",
        [{
            # Verify equality when cuda graphs allowed.
            "enforce_eager": False,
        }])
    @pytest.mark.parametrize("per_test_common_llm_kwargs", [{}])
    @pytest.mark.parametrize(
        "baseline_llm_kwargs",
        [{
            "model": "facebook/opt-6.7b",
        }])
    @pytest.mark.parametrize("test_llm_kwargs",
        [{
            "target_model": "facebook/opt-6.7b",
            "draft_model": "facebook/opt-125m",
            "draft_size": 5,
        }])
    @pytest.mark.parametrize("batch_size", [8])
    @pytest.mark.parametrize("output_len", [32])
    @pytest.mark.parametrize("seed", [1])
    def test_spec_decode_cuda_graph(baseline_llm_generator, test_llm_generator,
                                    batch_size, output_len):
        """Verify spec decode equality when cuda graphs are enabled.
        """
>       run_greedy_equality_correctness_test(
            baseline_llm_generator,
            test_llm_generator,
            batch_size,
            max_output_len=output_len,
            force_output_len=True,
        )

tests/spec_decode/e2e/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/spec_decode/e2e/conftest.py:390: in run_greedy_equality_correctness_test
    baseline_batch_token_ids) = get_output_from_llm_generator(
tests/spec_decode/e2e/conftest.py:323: in get_output_from_llm_generator
    outputs = llm.generate(prompts, sampling_params, use_tqdm=True)
vllm/entrypoints/llm.py:165: in generate
    return self._run_engine(use_tqdm)
vllm/entrypoints/llm.py:185: in _run_engine
    step_outputs = self.llm_engine.step()
vllm/engine/llm_engine.py:643: in step
    return self._process_model_outputs(output, scheduler_outputs)
vllm/engine/llm_engine.py:597: in _process_model_outputs
    self._process_sequence_group_outputs(seq_group, outputs)
vllm/engine/llm_engine.py:468: in _process_sequence_group_outputs
    self._decode_sequence(seq, seq_group.sampling_params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <vllm.engine.llm_engine.LLMEngine object at 0x7f7ea00c3130>
seq = Sequence(seq_id=0, status=RUNNING, num_blocks=1)
prms = SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, t...True, max_tokens=32, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True)

    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:
        """Decodes the new token for a sequence."""
>       (new_tokens, new_output_text, prefix_offset,
         read_offset) = detokenize_incrementally(
             self.tokenizer,
             all_input_ids=seq.get_token_ids(),
             prev_tokens=seq.tokens,
             prefix_offset=seq.prefix_offset,
             read_offset=seq.read_offset,
             skip_special_tokens=prms.skip_special_tokens,
             spaces_between_special_tokens=prms.spaces_between_special_tokens,
        )
E       ValueError: too many values to unpack (expected 4)

vllm/engine/llm_engine.py:720: ValueError
----------------------------- Captured stdout call -----------------------------
running spec_decode
gpu memory used (GB): 0=0.45; 
Done waiting for free GPU memory on devices devices=[0] (threshold_bytes/2**30=2.0) dur_s=0.00
use_async=False
Creating baseline_or_test='test' LLM for test_name='test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]'. (baseline_kwargs if baseline_or_test == "baseline" else test_kwargs)={'enforce_eager': False, 'download_dir': '/mnt/sda/download', 'target_model': 'facebook/opt-6.7b', 'draft_model': 'facebook/opt-125m', 'draft_size': 5}
INFO 06-18 07:11:41 spec_decode_llm_engine.py:73] Initializing an LLM engine with config: model='facebook/opt-6.7b', tokenizer='facebook/opt-6.7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir='/mnt/sda/download', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)
hidden_States shape:  torch.Size([2048, 768])
selected_token_indices:  tensor([   7,   15,   23,   31,   39,   47,   55,   63,   71,   79,   87,   95,
         103,  111,  119,  127,  135,  143,  151,  159,  167,  175,  183,  191,
         199,  207,  215,  223,  231,  239,  247,  255,  263,  271,  279,  287,
         295,  303,  311,  319,  327,  335,  343,  351,  359,  367,  375,  383,
         391,  399,  407,  415,  423,  431,  439,  447,  455,  463,  471,  479,
         487,  495,  503,  511,  519,  527,  535,  543,  551,  559,  567,  575,
         583,  591,  599,  607,  615,  623,  631,  639,  647,  655,  663,  671,
         679,  687,  695,  703,  711,  719,  727,  735,  743,  751,  759,  767,
         775,  783,  791,  799,  807,  815,  823,  831,  839,  847,  855,  863,
         871,  879,  887,  895,  903,  911,  919,  927,  935,  943,  951,  959,
         967,  975,  983,  991,  999, 1007, 1015, 1023, 1031, 1039, 1047, 1055,
        1063, 1071, 1079, 1087, 1095, 1103, 1111, 1119, 1127, 1135, 1143, 1151,
        1159, 1167, 1175, 1183, 1191, 1199, 1207, 1215, 1223, 1231, 1239, 1247,
        1255, 1263, 1271, 1279, 1287, 1295, 1303, 1311, 1319, 1327, 1335, 1343,
        1351, 1359, 1367, 1375, 1383, 1391, 1399, 1407, 1415, 1423, 1431, 1439,
        1447, 1455, 1463, 1471, 1479, 1487, 1495, 1503, 1511, 1519, 1527, 1535,
        1543, 1551, 1559, 1567, 1575, 1583, 1591, 1599, 1607, 1615, 1623, 1631,
        1639, 1647, 1655, 1663, 1671, 1679, 1687, 1695, 1703, 1711, 1719, 1727,
        1735, 1743, 1751, 1759, 1767, 1775, 1783, 1791, 1799, 1807, 1815, 1823,
        1831, 1839, 1847, 1855, 1863, 1871, 1879, 1887, 1895, 1903, 1911, 1919,
        1927, 1935, 1943, 1951, 1959, 1967, 1975, 1983, 1991, 1999, 2007, 2015,
        2023, 2031, 2039, 2047], device='cuda:0')
INFO 06-18 07:11:55 spec_decode_llm_engine.py:165] # GPU blocks: 3324, # CPU blocks: 478
INFO 06-18 07:11:57 spec_decode_model_runner.py:682] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-18 07:11:57 spec_decode_model_runner.py:686] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 06-18 07:12:00 spec_decode_model_runner.py:729] Graph capturing finished in 3 secs.
hidden_States shape:  torch.Size([64, 768])
selected_token_indices:  tensor([ 6, 15, 22, 29, 37, 45, 52, 62], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 7, 8], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([0, 2, 3, 4, 5, 6, 8, 9], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([ 0,  2,  4,  5,  6,  7,  9, 10], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([ 0,  2,  4,  5,  7,  9, 11, 12], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([ 0,  2,  4,  5,  7,  9, 11], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([1, 3, 5, 7, 9], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([16, 768])
selected_token_indices:  tensor([1, 3, 5, 7, 9], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([0, 1, 2, 3, 4], device='cuda:0')
hidden_States shape:  torch.Size([8, 768])
selected_token_indices:  tensor([1, 3, 5], device='cuda:0')
hidden_States shape:  torch.Size([4, 768])
selected_token_indices:  tensor([0, 1, 2], device='cuda:0')
hidden_States shape:  torch.Size([4, 768])
selected_token_indices:  tensor([0, 1, 2], device='cuda:0')
hidden_States shape:  torch.Size([4, 768])
selected_token_indices:  tensor([0, 1, 2], device='cuda:0')
hidden_States shape:  torch.Size([4, 768])
selected_token_indices:  tensor([0, 1, 2], device='cuda:0')
hidden_States shape:  torch.Size([4, 768])
selected_token_indices:  tensor([0, 2], device='cuda:0')
hidden_States shape:  torch.Size([2, 768])
selected_token_indices:  tensor([0, 1], device='cuda:0')
hidden_States shape:  torch.Size([2, 768])
selected_token_indices:  tensor([0, 1], device='cuda:0')
hidden_States shape:  torch.Size([2, 768])
selected_token_indices:  tensor([0, 1], device='cuda:0')
hidden_States shape:  torch.Size([2, 768])
selected_token_indices:  tensor([0, 1], device='cuda:0')
hidden_States shape:  torch.Size([2048, 4096])
selected_token_indices:  tensor([   7,   15,   23,   31,   39,   47,   55,   63,   71,   79,   87,   95,
         103,  111,  119,  127,  135,  143,  151,  159,  167,  175,  183,  191,
         199,  207,  215,  223,  231,  239,  247,  255,  263,  271,  279,  287,
         295,  303,  311,  319,  327,  335,  343,  351,  359,  367,  375,  383,
         391,  399,  407,  415,  423,  431,  439,  447,  455,  463,  471,  479,
         487,  495,  503,  511,  519,  527,  535,  543,  551,  559,  567,  575,
         583,  591,  599,  607,  615,  623,  631,  639,  647,  655,  663,  671,
         679,  687,  695,  703,  711,  719,  727,  735,  743,  751,  759,  767,
         775,  783,  791,  799,  807,  815,  823,  831,  839,  847,  855,  863,
         871,  879,  887,  895,  903,  911,  919,  927,  935,  943,  951,  959,
         967,  975,  983,  991,  999, 1007, 1015, 1023, 1031, 1039, 1047, 1055,
        1063, 1071, 1079, 1087, 1095, 1103, 1111, 1119, 1127, 1135, 1143, 1151,
        1159, 1167, 1175, 1183, 1191, 1199, 1207, 1215, 1223, 1231, 1239, 1247,
        1255, 1263, 1271, 1279, 1287, 1295, 1303, 1311, 1319, 1327, 1335, 1343,
        1351, 1359, 1367, 1375, 1383, 1391, 1399, 1407, 1415, 1423, 1431, 1439,
        1447, 1455, 1463, 1471, 1479, 1487, 1495, 1503, 1511, 1519, 1527, 1535,
        1543, 1551, 1559, 1567, 1575, 1583, 1591, 1599, 1607, 1615, 1623, 1631,
        1639, 1647, 1655, 1663, 1671, 1679, 1687, 1695, 1703, 1711, 1719, 1727,
        1735, 1743, 1751, 1759, 1767, 1775, 1783, 1791, 1799, 1807, 1815, 1823,
        1831, 1839, 1847, 1855, 1863, 1871, 1879, 1887, 1895, 1903, 1911, 1919,
        1927, 1935, 1943, 1951, 1959, 1967, 1975, 1983, 1991, 1999, 2007, 2015,
        2023, 2031, 2039, 2047], device='cuda:0')
hidden_States shape:  torch.Size([55, 4096])
selected_token_indices:  tensor([ 5, 13, 19, 25, 32, 39, 45, 54], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([48, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')
hidden_States shape:  torch.Size([42, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41], device='cuda:0')
hidden_States shape:  torch.Size([30, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], device='cuda:0')
hidden_States shape:  torch.Size([30, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], device='cuda:0')
hidden_States shape:  torch.Size([18, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],
       device='cuda:0')
hidden_States shape:  torch.Size([12, 4096])
selected_token_indices:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], device='cuda:0')
[" Inigo Montoya. I killed your father. Prepare to die.\nI'm sorry, but I'm not sure what you're talking about.I'm not sure if this", ' a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a', " Paris.\nI think you mean Paris, France.I'm not sure if this is a good thing or a bad thing.\nI think it's a good", ' in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in', ' tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech tech', ' Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark Mark', ' monkey.\nA monkey is a monkey.\nA monkey is a monkey.\nA monkey is a monkey.\nA monkey is a monkey.\nA monkey is', ' Python 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3']
running baseline
gpu memory used (GB): 0=41.08; 
gpu memory used (GB): 0=1.64; 
Done waiting for free GPU memory on devices devices=[0] (threshold_bytes/2**30=2.0) dur_s=5.01
use_async=False
Creating baseline_or_test='baseline' LLM for test_name='test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]'. (baseline_kwargs if baseline_or_test == "baseline" else test_kwargs)={'enforce_eager': False, 'download_dir': '/mnt/sda/download', 'model': 'facebook/opt-6.7b'}
INFO 06-18 07:12:07 llm_engine.py:70] Initializing an LLM engine with config: model='facebook/opt-6.7b', tokenizer='facebook/opt-6.7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir='/mnt/sda/download', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
hidden_States shape:  torch.Size([512, 4096])
selected_token_indices:  tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,
         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,
         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,
         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,
        113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139,
        141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167,
        169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195,
        197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223,
        225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251,
        253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279,
        281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307,
        309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335,
        337, 339, 341, 343, 345, 347, 349, 351, 353, 355, 357, 359, 361, 363,
        365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389, 391,
        393, 395, 397, 399, 401, 403, 405, 407, 409, 411, 413, 415, 417, 419,
        421, 423, 425, 427, 429, 431, 433, 435, 437, 439, 441, 443, 445, 447,
        449, 451, 453, 455, 457, 459, 461, 463, 465, 467, 469, 471, 473, 475,
        477, 479, 481, 483, 485, 487, 489, 491, 493, 495, 497, 499, 501, 503,
        505, 507, 509, 511], device='cuda:0')
INFO 06-18 07:12:15 llm_engine.py:275] # GPU blocks: 3714, # CPU blocks: 512
INFO 06-18 07:12:17 model_runner.py:508] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-18 07:12:17 model_runner.py:512] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 06-18 07:12:21 model_runner.py:555] Graph capturing finished in 4 secs.
[<vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f580>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f520>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f550>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f5b0>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f610>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f640>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f670>, <vllm.sequence.SequenceGroupMetadata object at 0x7f7e8ff4f6a0>]
hidden_States shape:  torch.Size([55, 4096])
selected_token_indices:  tensor([ 5, 13, 19, 25, 32, 39, 45, 54], device='cuda:0')
----------------------------- Captured stderr call -----------------------------
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s]Processed prompts:  12%|█▎        | 1/8 [00:00<00:02,  2.38it/s]Processed prompts:  50%|█████     | 4/8 [00:00<00:00,  7.86it/s]Processed prompts:  88%|████████▊ | 7/8 [00:00<00:00, 12.54it/s]Processed prompts: 100%|██████████| 8/8 [00:00<00:00, 11.21it/s]
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/sda/vllm/vllm/engine/worker_executor.py", line 105, in init_worker
    method, args, kwargs = pipe.recv()
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s]
=============================== warnings summary ===============================
tests/spec_decode/e2e/test_integration.py::test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]
tests/spec_decode/e2e/test_integration.py::test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]
  /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
    warnings.warn(

tests/spec_decode/e2e/test_integration.py::test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]
  /usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
    import distutils.core

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/spec_decode/e2e/test_integration.py::test_spec_decode_cuda_graph[1-32-8-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]
======================== 1 failed, 3 warnings in 40.60s ========================

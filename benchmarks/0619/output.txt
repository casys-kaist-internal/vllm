Namespace(dataset='finance', input_len=None, output_len=None, target_model='facebook/opt-13b', draft_model='facebook/opt-125m', draft_size=4, collocate=False, chunked_prefill=False, target_attention=True, tokenizer='facebook/opt-13b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, request_rate=4.0)
----------------  -----------------
target_model      facebook/opt-13b
draft_model       facebook/opt-125m
draft_size        4
collocate         False
chunked_prefill   False
target_attention  True
dataset           finance
----------------  -----------------
INFO 06-20 11:23:51 spec_decode_llm_engine.py:73] Initializing an LLM engine with config: model='facebook/opt-13b', tokenizer='facebook/opt-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir='/mnt/sda/download', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)
INFO 06-20 11:24:15 spec_decode_llm_engine.py:166] # GPU blocks: 1268, # CPU blocks: 313
INFO 06-20 11:24:17 spec_decode_model_runner.py:701] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-20 11:24:17 spec_decode_model_runner.py:705] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 06-20 11:24:21 spec_decode_model_runner.py:751] Graph capturing finished in 4 secs.
Warming up...
